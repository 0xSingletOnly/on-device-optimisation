# On Device Optimisation
Warning: Work in progress!!!

Trying to figure out what is the most efficient way that I can run a LLM locally on my current stack, which includes:
- MacBook Pro M1 16GB

## Steps
1. Run HuggingFace Transformer library and see what is the smallest model that I can deploy on my MacBook. I also need a benchmark to measure the inference speed.
2. 
